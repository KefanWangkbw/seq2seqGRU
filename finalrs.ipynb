import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import re

# Load the dataset
file_path = '/mnt/data/TED_Talks_dataset/TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv'
data = pd.read_csv(file_path)

# Preprocessing function to clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove multiple spaces
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

# Applying the cleaning function to the transcripts and titles
data['clean_transcript'] = data['transcript'].apply(clean_text)
data['clean_title'] = data['headline'].apply(clean_text)

# Tokenization and padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['clean_transcript'])
X = tokenizer.texts_to_sequences(data['clean_transcript'])
X = pad_sequences(X, maxlen=2500, truncating='post')

tokenizer.fit_on_texts(data['clean_title'])
y = tokenizer.texts_to_sequences(data['clean_title'])
y = pad_sequences(y, maxlen=15, truncating='post')

# Splitting the dataset
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import re

# Load the dataset
file_path = '/mnt/data/TED_Talks_dataset/TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv'
data = pd.read_csv(file_path)

# Preprocessing function to clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove multiple spaces
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

# Applying the cleaning function to the transcripts and titles
data['clean_transcript'] = data['transcript'].apply(clean_text)
data['clean_title'] = data['headline'].apply(clean_text)

# Tokenization and padding
tokenizer_transcript = Tokenizer()
tokenizer_transcript.fit_on_texts(data['clean_transcript'])
X = tokenizer_transcript.texts_to_sequences(data['clean_transcript'])
X = pad_sequences(X, maxlen=2500, truncating='post')

tokenizer_title = Tokenizer()
tokenizer_title.fit_on_texts(data['clean_title'])
y = tokenizer_title.texts_to_sequences(data['clean_title'])
y = pad_sequences(y, maxlen=15, truncating='post')

# Splitting the dataset
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Model parameters
latent_dim = 512
embedding_dim = 256
vocab_size_transcript = len(tokenizer_transcript.word_index) + 1
vocab_size_title = len(tokenizer_title.word_index) + 1

# ... Continue with model definition and training
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, GRU, Embedding, Dense, TimeDistributed

# Model parameters
latent_dim = 512
embedding_dim = 256
vocab_size = len(tokenizer.word_index) + 1

# Encoder
encoder_inputs = Input(shape=(None,))
enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)
encoder_gru = GRU(latent_dim, return_state=True)
encoder_outputs, state_h = encoder_gru(enc_emb)

# Decoder
decoder_inputs = Input(shape=(None,))
dec_emb_layer = Embedding(vocab_size, embedding_dim)
dec_emb = dec_emb_layer(decoder_inputs)
decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _ = decoder_gru(dec_emb, initial_state=state_h)
decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))
decoder_outputs = decoder_dense(decoder_outputs)

# Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Training
model.fit([X_train, y_train[:, :-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:], 
          batch_size=32, epochs=10)

from rouge import Rouge

# Function to generate summaries
def generate_summary(input_seq):
    # Similar to the decode_sequence function in the earlier code
    # Needs to be implemented based on the trained model
    pass

# Generate summaries for the validation set
predicted_summaries = [generate_summary(seq) for seq in X_val]

# Convert the sequences back to text
original_summaries = tokenizer.sequences_to_texts(y_val)
predicted_summaries_text = [' '.join(summary) for summary in predicted_summaries]

# Evaluate with ROUGE
rouge = Rouge()
scores = rouge.get_scores(predicted_summaries_text, original_summaries, avg=True)
print(scores)
